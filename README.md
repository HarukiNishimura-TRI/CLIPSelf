# CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction

## Introduction

This is an official release of the paper **CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**.

> [**CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction**](https://arxiv.org/abs/2302.13996),            
> Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, Chen Change Loy            
> [[project page(TBD)](https://www.mmlab-ntu.com/)][[Bibetex](https://github.com/wusize/CLIPSelf#citation)]

## TODO (Coming Soon)
- [ ] Code and models of CLIPSelf.
- [ ] CLIP models refined by CLIPSelf.
- [ ] Support F-ViT under the OVDet repo using MMDetection3.x.

## Installation

This project is adapted from [OpenCLIP-v2.16.0](https://github.com/mlfoundations/open_clip/tree/v2.16.0). Run the
following command to install the package

```bash
pip install -e. -v
```
## Train CLIPSelf

## Evaluate CLIPSelf

## F-ViT


## License

This project is released under the [Apache 2.0 license](LICENSE).


## Citation

```bibtex
@article{wu2023clipself,
    title={CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction},
    author={Size Wu and Wenwei Zhang and Lumin Xu and Sheng Jin and and Xiangtai Li and Wentao Liu and Chen Change Loy},
    journal={arXiv preprint arXiv:2303.13495},
    booktitle={CVPR},
}
```


## Acknowledgement

We thank [OpenCLIP](https://github.com/mlfoundations/open_clip/tree/v2.16.0) and 
[EVA-CLIP](https://github.com/baaivision/EVA/tree/master/EVA-CLIP) for their valuable code bases.